#!/usr/bin/env ruby
#
#  Usage:
#    export SFW_BASE_DOMAIN=my-fed-wiki-farm.org
#    bundle exec bin/oyp [url to crawl]
#
#  For local development, try spinning up a SFW (running in farm mode) on port 1111, then:
#    export SFW_BASE_DOMAIN=lvh.me:1111
#    bundle exec bin/oyp [url to crawl]
#
#  If no "url to crawl" is given, we look for a local file called 'links.txt' and crawl each link in turn
#  For a test URL, try http://en.wikipedia.org/wiki/Special:Random
#

require 'anemone'
require 'json'

Dir[File.expand_path("../lib/**/*.rb", File.dirname(__FILE__))].each { |lib| require lib }

MAX_LINKS_PER_SITE = (ENV['MAX_LINKS_PER_SITE'] || 2).to_i

def crawl(starting_url)
  html = RestClient.get starting_url rescue nil
  doc = Nokogiri::HTML(html)
  if html && (fork_url = FedWiki.open(doc, starting_url))
    puts "Created fedwiki page -->"
    puts
    puts fork_url
    visited = [starting_url]
    attempted = 0
    Anemone.crawl(starting_url) do |anemone|
      #anemone.focus_crawl { |page| page.links.slice(0...MAX_LINKS_PER_SITE) }
      anemone.on_every_page do |page|
        url = page.url.to_s
        unless visited.include?(url)
          #puts '~'*20
          sleep rand*16   # Note: Anemone runs 4 threads by default
          #sleep rand*8 # Note: Anemone runs 4 threads by default
          doc = page.doc
          if fork_url = FedWiki.open(doc, url)
            puts "Created fedwiki page -->"
            puts
            puts fork_url
            visited << url
          end
        end
        attempted += 1
        return if (visited.size >= MAX_LINKS_PER_SITE) || (attempted >= MAX_LINKS_PER_SITE * 2)
      end
    end
  end
end

### The action: read args and crawl --

raise "Please set the environment variable 'SFW_BASE_DOMAIN'" if ENV['SFW_BASE_DOMAIN'].nil? || ENV['SFW_BASE_DOMAIN'].empty?

url = ARGV.first
if url
  url = url =~ %r{^https?://} ? url : "http://#{url}"
  crawl url
else
  File.readlines('links.txt').each do |line|
    url = line.strip
    crawl url unless url.empty?
  end
end
