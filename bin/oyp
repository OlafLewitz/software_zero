#!/usr/bin/env ruby
#
#  Usage:
#    export SFW_BASE_DOMAIN=my-fed-wiki-farm.org
#    bundle exec bin/oyp [url to crawl]
#
#  For local development, try spinning up a SFW (running in farm mode) on port 1111, then:
#    export SFW_BASE_DOMAIN=lvh.me:1111
#    bundle exec bin/oyp [url to crawl]
#
#  If no "url to crawl" is given, we look for a local file called 'links.txt' and crawl each link in turn
#  For a test URL, try http://en.wikipedia.org/wiki/Special:Random
#

require 'anemone'
require 'html_massage'
require 'json'
require 'pismo'
require 'rest_client'

$LOAD_PATH.unshift(File.expand_path('../lib', (File.dirname(__FILE__))))
require 'ruby_extensions/string'
require 'sfw/random_id'
require 'forkdiffmerge/fork'

SUBDOMAIN_PATTERN = "[a-z0-9][a-z0-9-]{1,62}" # subdomains max at 63 characters
MAX_LINKS_PER_SITE = ( ENV['MAX_LINKS_PER_SITE'] || 2 ).to_i

def crawl(starting_url)
  Anemone.crawl(starting_url) do |anemone|
    anemone.focus_crawl { |page| page.links.slice(0...MAX_LINKS_PER_SITE) }
    anemone.on_every_page do |page|
      #puts '~'*20
      sleep rand*16   # Note: Anemone runs 4 threads by default
      doc = page.doc
      url = page.url.to_s
      fork_content doc, url
    end
  end
end

def run(cmd)
  cmd.gsub! /\s+/, ' '
  cmd.strip!
  puts "Running => #{cmd.inspect}"
  system cmd
end

def sfw_do(sfw_action_url, action, sfw_page_data)
  action_json = JSON.pretty_generate 'type' => action, 'item' => sfw_page_data
  RestClient.put "#{sfw_action_url}", :action => action_json, :content_type => :json, :accept => :json
end

### The action: read args and crawl --

unless ENV['SFW_BASE_DOMAIN']
  puts "Please set the environment variable 'SFW_BASE_DOMAIN'"
  exit 1
end

url = ARGV.first
if url
  url = url =~ %r{^https?://} ? url : "http://#{url}"
  crawl url
else
  File.readlines('links.txt').each do |line|
    p url = line.strip
    crawl url unless url.empty?
  end
end
