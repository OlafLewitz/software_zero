#!/usr/bin/env ruby
#
#  Usage:
#    bundle exec foreman run bin/zero [url to crawl]
#
#  For a test URL, try http://en.wikipedia.org/wiki/Special:Random
#  If no "url to crawl" is given, we look for a local file called 'links.txt' and crawl each link in turn
#

require 'anemone'
require 'json'

require_relative "../lib/env"
require_relative "../lib/fork_this/open"
require_relative "../app/models/page"


MAX_LINKS_PER_SITE = (Env['MAX_LINKS_PER_SITE'] || 2).to_i

Store.set 'GithubStore', File.expand_path("..", File.dirname(__FILE__))

def run
  raise "Please set the environment variable 'SFW_BASE_DOMAIN'" if Env['SFW_BASE_DOMAIN'].nil? || Env['SFW_BASE_DOMAIN'].empty?
  url = ARGV.first
  if url
    url = url =~ %r{^https?://} ? url : "http://#{url}"
    crawl url
  else
    File.readlines('links.txt').each do |line|
      url = line.strip
      crawl url unless url.empty?
    end
  end
end

def crawl(starting_url)
  html = RestClient.get starting_url rescue nil
  doc = Nokogiri::HTML(html)
  if html && !ForkThis.open_license_links(doc).empty?
    visited = {starting_url => doc}
    attempted = 1
    Anemone.crawl(starting_url) do |anemone|
      anemone.on_every_page do |page|
        sleep rand*4*4
        print '.'
        url = page.url.to_s
        visited[url] = page.doc  # TODO: if response 200
        attempted += 1
        if (visited.keys.size >= MAX_LINKS_PER_SITE) || (attempted >= MAX_LINKS_PER_SITE * 3)
          open_site(visited)
          return
        end
        #sleep rand*8
      end
    end
    open_site(visited)
  end
  puts
end

def open_site(visited)
  puts
  ForkThis.open_site(
    visited,
    :domain_connector => Env['DOMAIN_CONNECTOR'],
    :shorten_origin_domain => Env['SHORTEN_ORIGIN_DOMAIN']
  )
end


run()