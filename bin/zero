#!/usr/bin/env ruby
#
#  Usage:
#    export SFW_BASE_DOMAIN=my-fed-wiki-farm.org
#    bundle exec bin/oyp [url to crawl]
#
#  For local development, try spinning up a SFW (running in farm mode) on port 1111, then:
#    export SFW_BASE_DOMAIN=lvh.me:1111
#    bundle exec bin/oyp [url to crawl]
#
#  If no "url to crawl" is given, we look for a local file called 'links.txt' and crawl each link in turn
#  For a test URL, try http://en.wikipedia.org/wiki/Special:Random
#

require 'anemone'
require 'json'

Dir[File.expand_path("../lib/fedwiki/*.rb", File.dirname(__FILE__))].each { |lib| require lib }
require File.expand_path("../lib/env", File.dirname(__FILE__))


MAX_LINKS_PER_SITE = (Env['MAX_LINKS_PER_SITE'] || 2).to_i

def run
  raise "Please set the environment variable 'SFW_BASE_DOMAIN'" if Env['SFW_BASE_DOMAIN'].nil? || Env['SFW_BASE_DOMAIN'].empty?
  url = ARGV.first
  if url
    url = url =~ %r{^https?://} ? url : "http://#{url}"
    crawl url
  else
    File.readlines('links.txt').each do |line|
      url = line.strip
      crawl url unless url.empty?
    end
  end
end

def crawl(starting_url)
  html = RestClient.get starting_url rescue nil
  doc = Nokogiri::HTML(html)
  if html && !FedWiki.open_license_links(doc).empty?
    visited = {starting_url => doc}
    attempted = 1
    Anemone.crawl(starting_url) do |anemone|
      anemone.on_every_page do |page|
        sleep rand*4*4
        print '.'
        url = page.url.to_s
        visited[url] = page.doc  # TODO: if response 200
        attempted += 1
        if (visited.keys.size >= MAX_LINKS_PER_SITE) || (attempted >= MAX_LINKS_PER_SITE * 3)
          fedwiki_open_site(visited)
          return
        end
        #sleep rand*8
      end
    end
    fedwiki_open_site(visited)
  end
  puts
end

def fedwiki_open_site(visited)
  puts
  FedWiki.open_site(
    visited,
    :domain_connector => Env['DOMAIN_CONNECTOR'],
    :shorten_origin_domain => Env['SHORTEN_ORIGIN_DOMAIN']
  )
end


run()